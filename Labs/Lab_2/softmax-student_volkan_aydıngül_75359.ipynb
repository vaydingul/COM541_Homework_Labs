{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP541 - LAB #2\n",
    "## Classifying MNIST digits with a softmax classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a softmax classifier to predict the digit presented in a given image.\n",
    "We will use the MNIST dataset for this task. Please first skim through the notebook. Then complete the following steps mentioned in the main function:\n",
    "\n",
    "1. minibatch\n",
    "2. init_params\n",
    "3. forward and backward propagation\n",
    "    * softmax_forw\n",
    "    * softmax_back_and_loss\n",
    "4. grad_check\n",
    "5. train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we install **Knet** and **MLDatasets** only to be able to use MNIST dataset and to import some functions for testing purposes.\n",
    "Please execute this cell and enter `y` to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m registry at `C:\\Users\\volkan\\.julia\\registries\\General`\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\volkan\\.julia\\environments\\v1.5\\Project.toml`\n",
      "\u001b[32m\u001b[1mNo Changes\u001b[22m\u001b[39m to `C:\\Users\\volkan\\.julia\\environments\\v1.5\\Manifest.toml`\n",
      "┌ Info: Precompiling MLDatasets [eb30cadb-4394-5ae3-aed4-317e484a6458]\n",
      "└ @ Base loading.jl:1278\n",
      "┌ Info: Adding MLDatasets\n",
      "└ @ Main In[1]:5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg; for p in [\"MLDatasets\"]; Pkg.add(p); end\n",
    "using Printf, Random, Test, Statistics\n",
    "using MLDatasets: MNIST\n",
    "\n",
    "@info \"Adding MLDatasets\"\n",
    "@test size.(MNIST.traindata(Float32)) == ((28, 28, 60000), (60000,))\n",
    "@test size.(MNIST.testdata(Float32)) == ((28, 28, 10000), (10000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Here you should implement `minibatch` function which takes raw input array `x` and gold labels arrays `y` and splits them into mini batches to be processed.\n",
    "Hints: you can check arrays size with `size` function and reshape them using `reshape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing minibatch function\n",
      "└ @ Main In[12]:26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    minibatch(x, y, bs=100)\n",
    "\n",
    "Return a list of minibatches [(xi,yi)...] given data tensors x, y and batchsize.\n",
    "\n",
    "The last dimension of x and y give the number of instances and should be equal.\n",
    "\"\"\"\n",
    "function minibatch(x, y, bs=100)\n",
    "    data = Any[]\n",
    "    length = size(x, 1);\n",
    "    final = length / bs; #chunking the dataset\n",
    "    size_x = size(x)\n",
    "    size_y = size(y)\n",
    "    size_x_new = collect(size_x); size_x_new[end] = bs;\n",
    "    size_y_new = collect(size_y); size_y_new[end] = bs;\n",
    "\n",
    "    x = reshape(x, :, size_x[end]) # Access only to number of instances without knowing the internal dimension\n",
    "    y = reshape(y, :, size_y[end]) # Access only to number of instances without knowing the internal dimension\n",
    "    \n",
    "        # Minibatching operation with re-reshaping the data into original size\n",
    "    data = [(reshape(x[:, (k-1)*bs + 1: k*bs],Tuple(size_x_new)), reshape(y[:, (k-1)*bs + 1: k*bs], Tuple(size_y_new))) for k in 1:bs]\n",
    "    \n",
    "    return data\n",
    "end\n",
    "\n",
    "@info \"Testing minibatch function\"\n",
    "@test mean(minibatch(MNIST.testdata(Float32)...)[1][1]) ≈ 0.11988331\n",
    "@test size.(minibatch(MNIST.testdata(Float32)...)[100]) == ((28, 28, 100), (100,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Here you should implement `init_params` function which will be used to produce the initial values of the weights.\n",
    "Hints : look up `randn` and `zeros` functions using `@doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing init_params function\n",
      "└ @ Main In[13]:14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    init_params(ninputs, noutputs)\n",
    "\n",
    "Return a tuple of randomly generated W(sampled from N(0, 1e-3)) and b(must be zeros vector) params of softmax.\n",
    "\"\"\"\n",
    "function init_params(ninputs::Int, noutputs::Int)\n",
    "    w = randn(noutputs, ninputs) * 1e-3;\n",
    "    b = zeros(noutputs,1)\n",
    "    return w,b\n",
    "end\n",
    "Random.seed!(1)\n",
    "W, b = init_params(150, 100)\n",
    "\n",
    "@info \"Testing init_params function\"\n",
    "@test mean(W) ≈ 2.6869455422978772e-6\n",
    "@test size(b) == (100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "\n",
    "This function will takes three arguments, model weights `w`, `b` and a single minibatch input data `x`, applies the affine transformation and softmax function and returns predicted probabilities.\n",
    "After applying the affine transformation by multplying the input minibatch with the weights and adding the bias vector, you need to implement softmax function as follows:\n",
    "\n",
    "\\begin{eqnarray}{\\displaystyle P(y=j\\mid \\mathbf {x} )={\\frac {e^{\\mathbf {x} \\mathbf {w} _{j} + \\mathbf {b} _{j}}}{\\sum _{k=1}^{K}e^{\\mathbf {x} \\mathbf {w} _{j} + \\mathbf {b} _{k}}}}}\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing softmax_forw\n",
      "└ @ Main In[14]:17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    softmax_forw(W, b, x)\n",
    "\n",
    "Return the predicted probabilities of softmax function\n",
    "\"\"\"\n",
    "function softmax_forw(W, b, x)\n",
    "    logits = W * x .+ b\n",
    "    logits .-= maximum(logits);\n",
    "    probs = exp.(logits)\n",
    "    probs ./= sum(probs, dims = 1)\n",
    "    return probs\n",
    "end\n",
    "\n",
    "Random.seed!(1)\n",
    "W, b = init_params(150, 10)\n",
    "\n",
    "@info \"Testing softmax_forw\"\n",
    "@test sum(softmax_forw(W, b, randn(150, 32))[:, 1]) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2\n",
    "In this function you should firstly do the forward pass using the previous function that you implemented, after that you should calculate negative log likelihood loss:\n",
    "\n",
    "\\begin{eqnarray}{\\widehat {\\ell \\,}}(w, b \\,;x)={\\frac {-1}{n}}\\sum _{i=1}^{n} y_{i}\\ln f(x_{i}\\mid w,b )\\end{eqnarray}\n",
    "And then you should calculate the gradients of w and b and return them with the loss value.\n",
    "functions you may use: `log`, `sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1mIn[16]:28\u001b[22m\n",
      "  Expression: mean(gradw) ≈ 2.7582103268031233e-19\n",
      "   Evaluated: 5.568462357885551e-19 ≈ 2.7582103268031233e-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing softmax_back_and_loss\n",
      "└ @ Main In[16]:26\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "There was an error during testing",
     "output_type": "error",
     "traceback": [
      "There was an error during testing",
      "",
      "Stacktrace:",
      " [1] record(::Test.FallbackTestSet, ::Union{Test.Error, Test.Fail}) at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Test\\src\\Test.jl:737",
      " [2] do_test(::Test.ExecutionResult, ::Any) at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Test\\src\\Test.jl:520",
      " [3] top-level scope at In[16]:28",
      " [4] include_string(::Function, ::Module, ::String, ::String) at .\\loading.jl:1091",
      " [5] execute_code(::String, ::String) at C:\\Users\\volkan\\.julia\\packages\\IJulia\\rWZ9e\\src\\execute_request.jl:27",
      " [6] execute_request(::ZMQ.Socket, ::IJulia.Msg) at C:\\Users\\volkan\\.julia\\packages\\IJulia\\rWZ9e\\src\\execute_request.jl:86",
      " [7] #invokelatest#1 at .\\essentials.jl:710 [inlined]",
      " [8] invokelatest at .\\essentials.jl:709 [inlined]",
      " [9] eventloop(::ZMQ.Socket) at C:\\Users\\volkan\\.julia\\packages\\IJulia\\rWZ9e\\src\\eventloop.jl:8",
      " [10] (::IJulia.var\"#15#18\")() at .\\task.jl:356"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    softmax_back_and_loss(W, b, x, ygold)\n",
    "\n",
    "Do softmax forward pass and Return the loss and the gradients of w and b.\n",
    "\"\"\"\n",
    "function softmax_back_and_loss(W, b, x, ygold)\n",
    "    probs = softmax_forw(W, b, x);\n",
    "    class = argmax(ygold, dims = 1)\n",
    "    loss = sum(-log.(probs[class])) ./ size(ygold, 2)\n",
    "    \n",
    "    gradf = probs\n",
    "    gradf[class] .-= 1\n",
    "    gradf /= size(ygold, 2)\n",
    "\n",
    "    gradw = (gradf * x') \n",
    "    gradb = vec(sum(gradf, dims=2))\n",
    "    return loss, gradw, gradb\n",
    "end\n",
    "\n",
    "Random.seed!(1)\n",
    "x = randn(150, 32)\n",
    "ygold = zeros(10, 32)\n",
    "ygold[1, :] .= 1\n",
    "loss, gradw, gradb = softmax_back_and_loss(W, b, x, ygold)\n",
    "\n",
    "@info \"Testing softmax_back_and_loss\"\n",
    "@test loss ≈ 2.302861396444973\n",
    "@test mean(gradw) ≈ 2.7582103268031233e-19\n",
    "#@test isapprox(mean(gradw), 2.7582103268031233e-19, atol= 1e-10)\n",
    "@test mean(gradb) ≈ -1.3877787807814458e-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Given model weights `W` and `b`, and one minibatch input data `x` and true labels `ygold` as input parameters, this function should display info about whether your gradient calculation procedure passes the gradient check test or not.\n",
    "Your part of this function is to implement the `numeric_gradient` function, which should calculate the numeric gradients `gw` and `gb` and return them.\n",
    "Hint: you'll need to do the calculation of the loss for each single value of the parameters twice.\n",
    "To calculate the numeric gradient of a function recall this:\n",
    "\n",
    "\\begin{eqnarray}f'(x)={\\frac {f(x+h)-f(x-h)}{2h}}\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing grad_check\n",
      "└ @ Main In[17]:72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 9.879098353252534e-10\n",
      "Gradient Checking Passed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    grad_check(W, b, x, ygold)\n",
    "\n",
    "Check the accuracy of gradients of `W` and `b` which are calculated by `softmax_back_and_loss` function.\n",
    "\n",
    "This function does that by comparison with the numeric gradients.\n",
    "\"\"\"\n",
    "function grad_check(W, b, x, ygold)\n",
    "    \"\"\"\n",
    "        numeric_gradient()\n",
    "\n",
    "    Return numeric gradients of model weights `(gw, gb)`\n",
    "    \"\"\"\n",
    "    function numeric_gradient()\n",
    "        epsilon = 0.00001\n",
    "\n",
    "        gw = zeros(size(W)) # gradient of W\n",
    "        \n",
    "        gb = zeros(size(b)) # gradient of b\n",
    "        \n",
    "\n",
    "        for k in 1:size(W,1)\n",
    "            b[k] += epsilon # Forward evaluation\n",
    "            lossplus, _, _ = softmax_back_and_loss(W, b, x, ygold)\n",
    "            \n",
    "            b[k] -= 2*epsilon # Backwward evaluation\n",
    "            lossnegative, _, _ = softmax_back_and_loss(W, b, x, ygold) \n",
    "\n",
    "            b[k] += epsilon # Original form\n",
    "            \n",
    "            gb[k] = (lossplus - lossnegative) / (2 * epsilon)\n",
    "            \n",
    "          for m in 1:size(W,2)\n",
    "                \n",
    "                W[k, m] += epsilon # Forward evaluation\n",
    "                lossplus, _, _ = softmax_back_and_loss(W, b, x, ygold)\n",
    "                \n",
    "                W[k, m] -= 2*epsilon # Backward evaluation\n",
    "                lossnegative, _, _ = softmax_back_and_loss(W, b, x, ygold) \n",
    "                \n",
    "                W[k,m] += epsilon # Original form\n",
    "                \n",
    "                gw[k, m] = (lossplus - lossnegative) / (2 * epsilon)\n",
    "\n",
    "\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        return gw, gb\n",
    "    end\n",
    "\n",
    "    _,gradW,gradB = softmax_back_and_loss(W, b, x, ygold)\n",
    "    \n",
    "    gw, gb = numeric_gradient()\n",
    "    diff = sqrt(sum((gradW - gw) .^ 2) + sum((gradB - gb) .^ 2))\n",
    "    println(\"Diff: $diff\")\n",
    "    if diff < 1e-7\n",
    "        println(\"Gradient Checking Passed\")\n",
    "        return true\n",
    "    else\n",
    "        println(\"Diff must be < 1e-7\")\n",
    "        return false\n",
    "    end\n",
    "end\n",
    "\n",
    "Random.seed!(1)\n",
    "W, b = init_params(150, 10)\n",
    "x = randn(150, 32)\n",
    "ygold = zeros(10, 32)\n",
    "ygold[1, :] .= 1\n",
    "\n",
    "@info \"Testing grad_check\"\n",
    "@test grad_check(W, b, x, ygold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Given model weights `W` and `b`, set of minibatches `data` and learning rate `lr` as input this function should iterate over\n",
    "the whole dataset and in each iteration the weights should be updated using the gradients obtained from `softmax_back_and_loss` function call.\n",
    "Remember the update step of gradient descent optimization algorithm:\n",
    "\n",
    "\\begin{eqnarray}w_{i}:=w_{i}-\\eta \\nabla w_{i}\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing train(W, b, data, lr=0.15) function\n",
      "└ @ Main In[18]:27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m\u001b[1mTest Failed\u001b[22m\u001b[39m at \u001b[39m\u001b[1mIn[18]:28\u001b[22m\n",
      "  Expression: train(W, b, [(randn(150, 32), ygold) for i = 1:5]) == 2.037075694591626\n",
      "   Evaluated: 2.0370756945916257 == 2.037075694591626\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "There was an error during testing",
     "output_type": "error",
     "traceback": [
      "There was an error during testing",
      "",
      "Stacktrace:",
      " [1] record(::Test.FallbackTestSet, ::Union{Test.Error, Test.Fail}) at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Test\\src\\Test.jl:737",
      " [2] do_test(::Test.ExecutionResult, ::Any) at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Test\\src\\Test.jl:520",
      " [3] top-level scope at In[18]:28",
      " [4] include_string(::Function, ::Module, ::String, ::String) at .\\loading.jl:1091",
      " [5] execute_code(::String, ::String) at C:\\Users\\volkan\\.julia\\packages\\IJulia\\rWZ9e\\src\\execute_request.jl:27",
      " [6] execute_request(::ZMQ.Socket, ::IJulia.Msg) at C:\\Users\\volkan\\.julia\\packages\\IJulia\\rWZ9e\\src\\execute_request.jl:86",
      " [7] #invokelatest#1 at .\\essentials.jl:710 [inlined]",
      " [8] invokelatest at .\\essentials.jl:709 [inlined]",
      " [9] eventloop(::ZMQ.Socket) at C:\\Users\\volkan\\.julia\\packages\\IJulia\\rWZ9e\\src\\eventloop.jl:8",
      " [10] (::IJulia.var\"#15#18\")() at .\\task.jl:356"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    train(W, b, data, lr=0.15)\n",
    "\n",
    "Update the models weights `W`, `b` using the `data` with learning rate of `lr` and Return the average loss.\n",
    "\"\"\"\n",
    "function train(W, b, data, lr=0.15)\n",
    "    totalcost = 0.0\n",
    "    numins = 0\n",
    "    for (x, y) in data\n",
    "        loss, gradw, gradb = softmax_back_and_loss(W, b, x, y)\n",
    "        \n",
    "        W .-= lr .* gradw\n",
    "        b .-= lr .* gradb\n",
    "        \n",
    "        totalcost += loss\n",
    "        numins += 1\n",
    "    end\n",
    "\n",
    "    avgcost = totalcost / numins\n",
    "end\n",
    "\n",
    "Random.seed!(1)\n",
    "W, b = init_params(150, 10)\n",
    "ygold = zeros(10, 32)\n",
    "ygold[1, :] .= 1\n",
    "\n",
    "@info \"Testing train(W, b, data, lr=0.15) function\"\n",
    "@test train(W, b, [(randn(150, 32), ygold) for i=1:5]) == 2.037075694591626\n",
    "@test mean(W) == -1.1840149302803448e-5\n",
    "@test mean(b) == -2.7755575615628915e-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't touch this cell. Read it carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing accuracy(ygold, ypred) function\n",
      "└ @ Main In[19]:18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    accuracy(ygold, ypred)\n",
    "\n",
    "Return accuracy true labels (ygold) and predicted scores as input for single minibatch.\n",
    "\"\"\"\n",
    "function accuracy(ygold, ypred)\n",
    "    correct = 0.0\n",
    "    for i=1:size(ygold, 2)\n",
    "        correct += findmax(ygold[:,i]; dims=1)[2] == findmax(ypred[:, i]; dims=1)[2] ? 1.0 : 0.0\n",
    "    end\n",
    "    return correct / size(ygold, 2)\n",
    "end\n",
    "\n",
    "Random.seed!(1)\n",
    "W, b = init_params(150, 10)\n",
    "ygold = zeros(10, 32)\n",
    "\n",
    "@info \"Testing accuracy(ygold, ypred) function\"\n",
    "@test accuracy(ygold, softmax_forw(W, b, randn(150, 32))) == 0.09375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't touch this cell. Read it carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function main()\n",
    "    Random.seed!(12345)\n",
    "\n",
    "    # Size of input vector (MNIST images are 28x28)\n",
    "    ninputs = 28 * 28\n",
    "\n",
    "    # Number of classes (MNIST images fall into 10 classes)\n",
    "    noutputs = 10\n",
    "\n",
    "    #### Data loading & preprocessing\n",
    "    #\n",
    "    #  In this section, we load the input and output data,\n",
    "    #  prepare data to feed into softmax model.\n",
    "    #  For softmax regression on MNIST pixels,\n",
    "    #  the input data is the images, and\n",
    "    #  the output data is the labels.\n",
    "    #  Size of xtrn: (28,28,60000)\n",
    "    #  Size of xtrn must be: (784, 60000)\n",
    "    #  Size of xtst must be: (784, 10000)\n",
    "    #  Size of ytrn must be: (10, 60000)\n",
    "    #  Size of ytst must be: (10, 10000)\n",
    "\n",
    "    xtrn,ytrn = MNIST.traindata(Float32); ytrn[ytrn.==0] .= 10\n",
    "    xtst,ytst = MNIST.testdata(Float32);  ytst[ytst.==0] .= 10\n",
    "    xtrn = reshape(xtrn, 784, 60000)\n",
    "    xtst = reshape(xtst, 784, 10000)\n",
    "    function to_onehot(x)\n",
    "        onehot = zeros(10, 1)\n",
    "        onehot[x, 1] = 1.0\n",
    "        return onehot\n",
    "    end\n",
    "\n",
    "    ytrn = hcat(map(to_onehot, ytrn)...)\n",
    "    ytst = hcat(map(to_onehot, ytst)...)\n",
    "\n",
    "    # STEP 1: Create minibatches\n",
    "    #   Complete the minibatch function\n",
    "    #   It takes the input matrix (X) and gold labels (Y)\n",
    "    #   returns list of tuples contain minibatched input and labels (x, y)\n",
    "    bs = 100\n",
    "    trn_data = minibatch(xtrn, ytrn, bs)\n",
    "\n",
    "    # STEP 2: Initialize parameters\n",
    "    #   Complete init_params function\n",
    "    #   It takes number of inputs and number of outputs(number of classes)\n",
    "    #   It returns randomly generated W matrix and bias vector\n",
    "    #   Sample from N(0, 0.001)\n",
    "\n",
    "    W, b = init_params(ninputs, noutputs)\n",
    "\n",
    "    # STEP 3: Implement softmax_forw and softmax_back_and_loss\n",
    "    #   softmax_forw function takes W, b, and data\n",
    "    #   calculates predicted probabilities\n",
    "    #\n",
    "    #   softmax_back_and_loss function obtains probabilites by calling\n",
    "    #   softmax_forw then calculates soft loss and gradients of W and b\n",
    "\n",
    "    # STEP 4: Gradient checking\n",
    "    #   Skip this part for the lab session.\n",
    "    #   As with any learning algorithm, you should always check that your\n",
    "    #   gradients are correct before learning the parameters.\n",
    "    debug = true # Turn this parameter off, after gradient checking passed\n",
    "    if debug\n",
    "        grad_check(W, b, xtrn[:, 1:100], ytrn[:, 1:100])\n",
    "    end\n",
    "\n",
    "    lr = 0.15\n",
    "\n",
    "    # STEP 5: Training\n",
    "    #   The train function takes model parameters and the data\n",
    "    #   Trains the model over minibatches\n",
    "    #   For each minibatch, first cost and gradients are calculated then model parameters are updated\n",
    "    #   train function returns the average cost per instance\n",
    "\n",
    "    for i=1:50\n",
    "        cost = train(W, b, trn_data, lr)\n",
    "        pred = softmax_forw(W, b, xtrn)\n",
    "        trnacc = accuracy(ytrn, pred)\n",
    "        pred = softmax_forw(W, b, xtst)\n",
    "        tstacc = accuracy(ytst, pred)\n",
    "        @printf(\"epoch: %d softloss: %g trn accuracy: %g tst accuracy: %g\\n\", i, cost, trnacc, tstacc)\n",
    "    end\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 9.390935718438783e-10\n",
      "Gradient Checking Passed\n",
      "epoch: 1 softloss: 0.823248 trn accuracy: 0.859417 tst accuracy: 0.8667\n",
      "epoch: 2 softloss: 0.462602 trn accuracy: 0.877583 tst accuracy: 0.8843\n",
      "epoch: 3 softloss: 0.40045 trn accuracy: 0.885583 tst accuracy: 0.8914\n",
      "epoch: 4 softloss: 0.368761 trn accuracy: 0.89005 tst accuracy: 0.8973\n",
      "epoch: 5 softloss: 0.348327 trn accuracy: 0.893717 tst accuracy: 0.9002\n",
      "epoch: 6 softloss: 0.333549 trn accuracy: 0.896367 tst accuracy: 0.9027\n",
      "epoch: 7 softloss: 0.322106 trn accuracy: 0.89815 tst accuracy: 0.9049\n",
      "epoch: 8 softloss: 0.312833 trn accuracy: 0.8998 tst accuracy: 0.9066\n",
      "epoch: 9 softloss: 0.305072 trn accuracy: 0.9014 tst accuracy: 0.9078\n",
      "epoch: 10 softloss: 0.298421 trn accuracy: 0.90295 tst accuracy: 0.9077\n",
      "epoch: 11 softloss: 0.292614 trn accuracy: 0.903833 tst accuracy: 0.9083\n",
      "epoch: 12 softloss: 0.28747 trn accuracy: 0.9046 tst accuracy: 0.9083\n",
      "epoch: 13 softloss: 0.282858 trn accuracy: 0.905233 tst accuracy: 0.9085\n",
      "epoch: 14 softloss: 0.278683 trn accuracy: 0.90585 tst accuracy: 0.9088\n",
      "epoch: 15 softloss: 0.274872 trn accuracy: 0.906317 tst accuracy: 0.9092\n",
      "epoch: 16 softloss: 0.271369 trn accuracy: 0.906667 tst accuracy: 0.9097\n",
      "epoch: 17 softloss: 0.268129 trn accuracy: 0.906917 tst accuracy: 0.91\n",
      "epoch: 18 softloss: 0.265116 trn accuracy: 0.907017 tst accuracy: 0.9105\n",
      "epoch: 19 softloss: 0.262302 trn accuracy: 0.90745 tst accuracy: 0.9103\n",
      "epoch: 20 softloss: 0.259663 trn accuracy: 0.9076 tst accuracy: 0.9106\n",
      "epoch: 21 softloss: 0.257178 trn accuracy: 0.907933 tst accuracy: 0.9109\n",
      "epoch: 22 softloss: 0.254831 trn accuracy: 0.908217 tst accuracy: 0.911\n",
      "epoch: 23 softloss: 0.252608 trn accuracy: 0.90845 tst accuracy: 0.9106\n",
      "epoch: 24 softloss: 0.250496 trn accuracy: 0.908967 tst accuracy: 0.9104\n",
      "epoch: 25 softloss: 0.248486 trn accuracy: 0.909333 tst accuracy: 0.9105\n",
      "epoch: 26 softloss: 0.246567 trn accuracy: 0.909483 tst accuracy: 0.9101\n",
      "epoch: 27 softloss: 0.244731 trn accuracy: 0.909667 tst accuracy: 0.9102\n",
      "epoch: 28 softloss: 0.242973 trn accuracy: 0.909883 tst accuracy: 0.91\n",
      "epoch: 29 softloss: 0.241286 trn accuracy: 0.910117 tst accuracy: 0.91\n",
      "epoch: 30 softloss: 0.239663 trn accuracy: 0.910283 tst accuracy: 0.9105\n",
      "epoch: 31 softloss: 0.238101 trn accuracy: 0.91055 tst accuracy: 0.9105\n",
      "epoch: 32 softloss: 0.236596 trn accuracy: 0.910683 tst accuracy: 0.9105\n",
      "epoch: 33 softloss: 0.235142 trn accuracy: 0.9108 tst accuracy: 0.9103\n",
      "epoch: 34 softloss: 0.233737 trn accuracy: 0.911 tst accuracy: 0.9106\n",
      "epoch: 35 softloss: 0.232378 trn accuracy: 0.911017 tst accuracy: 0.9107\n",
      "epoch: 36 softloss: 0.231061 trn accuracy: 0.91115 tst accuracy: 0.9109\n",
      "epoch: 37 softloss: 0.229784 trn accuracy: 0.911183 tst accuracy: 0.9111\n",
      "epoch: 38 softloss: 0.228545 trn accuracy: 0.911333 tst accuracy: 0.9113\n",
      "epoch: 39 softloss: 0.227341 trn accuracy: 0.911333 tst accuracy: 0.9112\n",
      "epoch: 40 softloss: 0.226171 trn accuracy: 0.911517 tst accuracy: 0.9114\n",
      "epoch: 41 softloss: 0.225033 trn accuracy: 0.911433 tst accuracy: 0.9115\n",
      "epoch: 42 softloss: 0.223924 trn accuracy: 0.911483 tst accuracy: 0.9114\n",
      "epoch: 43 softloss: 0.222844 trn accuracy: 0.911483 tst accuracy: 0.9116\n",
      "epoch: 44 softloss: 0.221791 trn accuracy: 0.911717 tst accuracy: 0.9115\n",
      "epoch: 45 softloss: 0.220764 trn accuracy: 0.9118 tst accuracy: 0.9114\n",
      "epoch: 46 softloss: 0.219761 trn accuracy: 0.911933 tst accuracy: 0.9116\n",
      "epoch: 47 softloss: 0.218781 trn accuracy: 0.912067 tst accuracy: 0.912\n",
      "epoch: 48 softloss: 0.217824 trn accuracy: 0.912117 tst accuracy: 0.9118\n",
      "epoch: 49 softloss: 0.216888 trn accuracy: 0.91215 tst accuracy: 0.9117\n",
      "epoch: 50 softloss: 0.215972 trn accuracy: 0.912233 tst accuracy: 0.9117\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "#= Example Output\n",
    "Diff: 1.8292339049184216e-9\n",
    "Gradient Checking Passed\n",
    "epoch: 1 softloss: 0.481559 trn accuracy: 0.896983 tst accuracy: 0.9064\n",
    "epoch: 2 softloss: 0.339105 trn accuracy: 0.907617 tst accuracy: 0.9119\n",
    "epoch: 3 softloss: 0.31604 trn accuracy: 0.912017 tst accuracy: 0.9142\n",
    "epoch: 4 softloss: 0.303876 trn accuracy: 0.914783 tst accuracy: 0.9156\n",
    "epoch: 5 softloss: 0.29597 trn accuracy: 0.916567 tst accuracy: 0.9172\n",
    "epoch: 6 softloss: 0.290259 trn accuracy: 0.918033 tst accuracy: 0.9187\n",
    "epoch: 7 softloss: 0.285858 trn accuracy: 0.919233 tst accuracy: 0.9198\n",
    "epoch: 8 softloss: 0.282317 trn accuracy: 0.920083 tst accuracy: 0.92\n",
    "epoch: 9 softloss: 0.279378 trn accuracy: 0.9209 tst accuracy: 0.9204\n",
    "epoch: 10 softloss: 0.276879 trn accuracy: 0.921717 tst accuracy: 0.9211\n",
    "epoch: 11 softloss: 0.274716 trn accuracy: 0.92225 tst accuracy: 0.9207\n",
    "epoch: 12 softloss: 0.272816 trn accuracy: 0.92305 tst accuracy: 0.9214\n",
    "epoch: 13 softloss: 0.271127 trn accuracy: 0.923667 tst accuracy: 0.9214\n",
    "epoch: 14 softloss: 0.269609 trn accuracy: 0.924133 tst accuracy: 0.9215\n",
    "epoch: 15 softloss: 0.268235 trn accuracy: 0.924417 tst accuracy: 0.922\n",
    "epoch: 16 softloss: 0.26698 trn accuracy: 0.9247 tst accuracy: 0.9219\n",
    "epoch: 17 softloss: 0.265828 trn accuracy: 0.924933 tst accuracy: 0.9218\n",
    "epoch: 18 softloss: 0.264764 trn accuracy: 0.92505 tst accuracy: 0.922\n",
    "epoch: 19 softloss: 0.263777 trn accuracy: 0.925367 tst accuracy: 0.9223\n",
    "epoch: 20 softloss: 0.262856 trn accuracy: 0.92575 tst accuracy: 0.9225\n",
    "epoch: 21 softloss: 0.261995 trn accuracy: 0.9263 tst accuracy: 0.9227\n",
    "epoch: 22 softloss: 0.261186 trn accuracy: 0.926567 tst accuracy: 0.9226\n",
    "epoch: 23 softloss: 0.260424 trn accuracy: 0.9269 tst accuracy: 0.9229\n",
    "epoch: 24 softloss: 0.259704 trn accuracy: 0.92715 tst accuracy: 0.9227\n",
    "epoch: 25 softloss: 0.259022 trn accuracy: 0.927367 tst accuracy: 0.9227\n",
    "epoch: 26 softloss: 0.258374 trn accuracy: 0.9275 tst accuracy: 0.9229\n",
    "epoch: 27 softloss: 0.257758 trn accuracy: 0.927767 tst accuracy: 0.923\n",
    "epoch: 28 softloss: 0.257171 trn accuracy: 0.928083 tst accuracy: 0.9229\n",
    "epoch: 29 softloss: 0.25661 trn accuracy: 0.92825 tst accuracy: 0.9231\n",
    "epoch: 30 softloss: 0.256073 trn accuracy: 0.92835 tst accuracy: 0.9229\n",
    "epoch: 31 softloss: 0.255558 trn accuracy: 0.928517 tst accuracy: 0.923\n",
    "epoch: 32 softloss: 0.255064 trn accuracy: 0.928783 tst accuracy: 0.9228\n",
    "epoch: 33 softloss: 0.254589 trn accuracy: 0.92895 tst accuracy: 0.9229\n",
    "epoch: 34 softloss: 0.254133 trn accuracy: 0.9291 tst accuracy: 0.9227\n",
    "epoch: 35 softloss: 0.253692 trn accuracy: 0.929167 tst accuracy: 0.9228\n",
    "epoch: 36 softloss: 0.253268 trn accuracy: 0.92925 tst accuracy: 0.9227\n",
    "epoch: 37 softloss: 0.252858 trn accuracy: 0.929417 tst accuracy: 0.923\n",
    "epoch: 38 softloss: 0.252462 trn accuracy: 0.929567 tst accuracy: 0.9229\n",
    "epoch: 39 softloss: 0.252078 trn accuracy: 0.929667 tst accuracy: 0.9228\n",
    "epoch: 40 softloss: 0.251707 trn accuracy: 0.929783 tst accuracy: 0.9229\n",
    "epoch: 41 softloss: 0.251347 trn accuracy: 0.929867 tst accuracy: 0.9231\n",
    "epoch: 42 softloss: 0.250998 trn accuracy: 0.930067 tst accuracy: 0.9235\n",
    "epoch: 43 softloss: 0.25066 trn accuracy: 0.9301 tst accuracy: 0.9235\n",
    "epoch: 44 softloss: 0.250331 trn accuracy: 0.930233 tst accuracy: 0.9235\n",
    "epoch: 45 softloss: 0.250011 trn accuracy: 0.930333 tst accuracy: 0.9235\n",
    "epoch: 46 softloss: 0.2497 trn accuracy: 0.9305 tst accuracy: 0.9237\n",
    "epoch: 47 softloss: 0.249397 trn accuracy: 0.930583 tst accuracy: 0.9238\n",
    "epoch: 48 softloss: 0.249102 trn accuracy: 0.9307 tst accuracy: 0.9239\n",
    "epoch: 49 softloss: 0.248815 trn accuracy: 0.93085 tst accuracy: 0.9242\n",
    "epoch: 50 softloss: 0.248535 trn accuracy: 0.930933 tst accuracy: 0.9243\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}